from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
from dotenv import load_dotenv
from typing import Dict

from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import InMemoryChatMessageHistory
from langchain_core.runnables import RunnableWithMessageHistory

from session_utils import (
    user_might_want_to_end,
    has_repeated_answers,
    user_gave_blank_responses,
)

load_dotenv()

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

store: Dict[str, Dict[str, object]] = {}

class ChatRequest(BaseModel):
    message: str

class ChatResponse(BaseModel):
    reply: str
    show_end_prompt: bool = False

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
)

prompt = ChatPromptTemplate.from_messages([
    ("system", "你是一个专业的半导体设备故障采访助手。请一步步引导用户描述问题现象、出现时机、相关操作和可能原因。"),
    MessagesPlaceholder(variable_name="messages"),
])

chain = prompt | llm

chain_with_history = RunnableWithMessageHistory(
    chain,
    lambda session_id: store.setdefault(session_id, {
        "history": InMemoryChatMessageHistory(),
        "asked_end": False,
        "answer_log": [],
        "user_inputs": []
    })["history"],
    input_messages_key="input",
    history_messages_key="messages"
)

@app.post("/chat/{session_id}", response_model=ChatResponse)
async def chat(session_id: str, req: ChatRequest):
    # 初始化 session
    if session_id not in store:
        store[session_id] = {
            "history": InMemoryChatMessageHistory(),
            "asked_end": False,
            "answer_log": [],
            "user_inputs": []
        }

    session = store[session_id]
    asked_end = session["asked_end"]
    answer_log = session["answer_log"]
    user_inputs = session["user_inputs"]

    # 更新用户输入记录（最多保留 3 条）
    user_inputs.append(req.message.strip())
    if len(user_inputs) > 3:
        user_inputs.pop(0)

    # 1️⃣ 用户说了类似“没了”的表达
    if not asked_end and user_might_want_to_end(req.message):
        session["asked_end"] = True
        return ChatResponse(
            reply="看起来您可能想结束采访，请问是否还有其他问题要补充？",
            show_end_prompt=True
        )

    # 2️⃣ 用户连续无效回答
    if not asked_end and user_gave_blank_responses(user_inputs, blank_threshold=3):
        session["asked_end"] = True
        return ChatResponse(
            reply="您好像暂时没有更多补充，如果需要，我们可以结束这次采访。",
            show_end_prompt=True
        )

    # 🔁 正常生成回答
    response = await chain_with_history.ainvoke(
        {"input": req.message},
        config={"configurable": {"session_id": session_id}},
    )
    answer_text = response.content.strip()
    answer_log.append(answer_text)

    # 3️⃣ 某个回答出现 3 次以上
    if not asked_end and has_repeated_answers(answer_log, threshold=3):
        session["asked_end"] = True
        return ChatResponse(
            reply="我注意到我们有些回答已经重复多次，请问是否需要结束本次采访？",
            show_end_prompt=True
        )

    return ChatResponse(reply=answer_text, show_end_prompt=False)
