# telnault_api.py
import os
import httpx
from typing import List, Optional
from langchain_core.language_models.chat_models import BaseChatModel
from langchain_core.messages import HumanMessage, AIMessage, BaseMessage, SystemMessage
from langchain_core.outputs import ChatResult
from langchain_core.language_models.llms import LLMResult

class ChatTELnautLLM(BaseChatModel):
    """自定义 LangChain LLM，用你公司的 GPT API"""

    def __init__(self):
        self.client = httpx.AsyncClient()
        self.api_url = os.environ["GPT4_MINI_URL"]
        self.headers = {
            "sid": os.environ["GPT4_MINI_SID"],
            "env": os.environ["GPT4_MINI_ENV"],
            "apikey": os.environ["GPT4_MINI_API_KEY"],
            "empno": os.environ.get("EMPNO", ""),
        }

    async def _agenerate(
        self,
        messages: List[BaseMessage],
        stop: Optional[List[str]] = None,
    ) -> ChatResult:
        prompt = "\n".join([msg.content for msg in messages if isinstance(msg, (HumanMessage, SystemMessage))])

        response = await self.client.post(
            self.api_url,
            json={"input": prompt},
            headers=self.headers,
        )
        output = response.json()["output"]
        return ChatResult(generations=[[AIMessage(content=output)]])
